{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import sklearn\n",
    "import scipy.sparse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/raw_data/kaggle/train.csv')\n",
    "labels=(data[\"toxic\"] | data[\"severe_toxic\"] | data[\"obscene\"] | data[\"threat\"] | data[\"insult\"] | data[\"identity_hate\"])\n",
    "# labels=data[\"Labels\"]\n",
    "train_data = data[0:100000]\n",
    "test_data = data[100001:]\n",
    "train_labels = labels[0:100000]\n",
    "test_labels = labels[100001:]\n",
    "\n",
    "corpus = data[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_words( corpus ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    new_corp = []\n",
    "    i=0\n",
    "    for rev in corpus:\n",
    "        i+=1\n",
    "        if(i%10000 == 0):\n",
    "            print(\"iter\", int(i/10000),\"of\", int(len(corpus)/10000))\n",
    "        \n",
    "        # 1. Remove HTML\n",
    "        review_text = BeautifulSoup(rev).get_text() \n",
    "        #\n",
    "        # 2. Remove non-letters        \n",
    "        letters_only = re.sub(\"[^a-zA-Z']\", \" \", review_text)\n",
    "\n",
    "        p = letters_only.split()\n",
    "        new_corp.append(\" \".join(p))\n",
    "    return(new_corp)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1.0 of 15.9571\n",
      "iter 2.0 of 15.9571\n",
      "iter 3.0 of 15.9571\n",
      "iter 4.0 of 15.9571\n",
      "iter 5.0 of 15.9571\n",
      "iter 6.0 of 15.9571\n",
      "iter 7.0 of 15.9571\n",
      "iter 8.0 of 15.9571\n",
      "iter 9.0 of 15.9571\n",
      "iter 10.0 of 15.9571\n",
      "iter 11.0 of 15.9571\n",
      "iter 12.0 of 15.9571\n",
      "iter 13.0 of 15.9571\n",
      "iter 14.0 of 15.9571\n",
      "iter 15.0 of 15.9571\n"
     ]
    }
   ],
   "source": [
    "corpus = list(corpus)\n",
    "corpu=review_to_words(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(strip_accents='unicode',lowercase=True,stop_words='english', analyzer=\"word\",token_pattern=r'\\w{1,}')\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analyze = vectorizer.build_analyzer()\n",
    "\n",
    "sentences = []\n",
    "for comment in corpus:    \n",
    "    sentences.append(analyze(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_matrix = model.wv.vectors\n",
    "word2vec_matrix_sparse = sparse.csr_matrix(word2vec_matrix)\n",
    "\n",
    "f = np.dot(X,word2vec_matrix_sparse)\n",
    "train_feature_vectors = f[0:100000]\n",
    "test_feature_vectors = f[100001:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                         multi_class='ovr').fit(train_feature_vectors, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(test_feature_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(test_feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision =  0.00944647000331\n",
      "Recall =  0.532710280374\n",
      "F_score =  0.0185637518319\n",
      "CCR =  0.898824911868\n",
      "\n",
      "Confusion Matrix: \n",
      " [[53486    50]\n",
      " [ 5977    57]]\n"
     ]
    }
   ],
   "source": [
    "confmat = confusion_matrix(test_labels, predictions)\n",
    "\n",
    "precision = confmat[1,1]/(confmat[1,1] + confmat[1,0])\n",
    "recall = confmat[1,1]/(confmat[1,1] + confmat[0,1])\n",
    "fScore = (2*precision*recall)/(recall+precision)\n",
    "ccr = (confmat[0,0] + confmat[1,1])/(sum(sum(confmat)))\n",
    "\n",
    "print(\"Precision = \",precision)\n",
    "print(\"Recall = \",recall)\n",
    "print(\"F_score = \",fScore)\n",
    "print(\"CCR = \",ccr)\n",
    "print()\n",
    "print(\"Confusion Matrix: \\n\", confmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
