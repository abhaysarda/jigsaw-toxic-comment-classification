{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhay\\Anaconda3\\envs\\toxic-comments\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "#from keras import initializations\n",
    "from keras import initializers, regularizers, constraints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        #self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "    #print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0],  self.features_dim\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/preprocessed_data/'\n",
    "EMBEDDING_FILE='./word_embeddings/glove.840B.300d.txt'\n",
    "TRAIN_DATA_FILE=path+'train.csv'\n",
    "TEST_DATA_FILE=path+'test.csv'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "MAX_NB_WORDS = 40000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.15\n",
    "\n",
    "num_lstm = 200\n",
    "num_dense = 256\n",
    "rate_drop_lstm = 0.4\n",
    "rate_drop_dense = 0.4\n",
    "num_epochs = 10\n",
    "act = 'relu'\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>\" \\r\\r\\r\\n == Speedy deletion of \"\"Mason Henso...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>onwhat ehbcdpyedwcdo vaeotgbcdjvfh8dwikxosmn b...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002</td>\n",
       "      <td>*I don't know what's going on here - the note ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1003</td>\n",
       "      <td>\" \\r\\r\\r\\n\\r\\r\\r\\n  \\r\\r\\r\\n == Final draft… m...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004</td>\n",
       "      <td>\" \\r\\r\\r\\n\\r\\r\\r\\n : This is not an edit propo...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1005</td>\n",
       "      <td>Is he contesting 2014 Lok Sabha elecction. If ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1006</td>\n",
       "      <td>== 'citation needed' is not needed == \\r\\r\\r\\n...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1007</td>\n",
       "      <td>\" \\r\\r\\r\\n\\r\\r\\r\\n  \\r\\r\\r\\n\\r\\r\\r\\n WOW, so t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1008</td>\n",
       "      <td>\" \\r\\r\\r\\n\\r\\r\\r\\n :Looking at the issue again...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1009</td>\n",
       "      <td>Hi David_FLXD. I am coming back to you with re...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1010</td>\n",
       "      <td>There was no warthog like that in halo 2, your...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1011</td>\n",
       "      <td>I am glad we have 2010–2011 Tunisian protests....</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1012</td>\n",
       "      <td>is clearly intended to make</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1013</td>\n",
       "      <td>Yes it's more than 1500 killed civils, and man...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1014</td>\n",
       "      <td>\" \\r\\r\\r\\n :In the example I showed you, the s...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1015</td>\n",
       "      <td>== Thas the reason for them to request despera...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1016</td>\n",
       "      <td>::There is now a mention in the Eastern Daily ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1017</td>\n",
       "      <td>== Sideways photo == \\r\\r\\r\\n\\r\\r\\r\\n Umm...bl...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1018</td>\n",
       "      <td>You will need WP:RS sources that directly rela...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1019</td>\n",
       "      <td>\" \\r\\r\\r\\n\\r\\r\\r\\n ::Thanks, Steinberger.   \"</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1020</td>\n",
       "      <td>2009 (UTC) \\r\\r\\r\\n ::::::I think your idea is...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1021</td>\n",
       "      <td>\", 26 July 2010 (UTC) \\r\\r\\r\\n ::Can they be p...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1022</td>\n",
       "      <td>OH DEAR - HOW TERRIBLE ! GIVEN THAT IT TAKES 1...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1023</td>\n",
       "      <td>\" \\r\\r\\r\\n :The page was basically copied from...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1024</td>\n",
       "      <td>You want to build a framework like that, find ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1025</td>\n",
       "      <td>Ethnologue SIL is not a reference concerning C...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1026</td>\n",
       "      <td>I have removed what appears to me to be a spur...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1027</td>\n",
       "      <td>== Hotels in Abuja == \\r\\r\\r\\n\\r\\r\\r\\n Wheneve...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1028</td>\n",
       "      <td>my opinion about the Nakhcivan  Nakhcivan is a...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1029</td>\n",
       "      <td>\" \\r\\r\\r\\n\\r\\r\\r\\n == Employees == \\r\\r\\r\\n\\r\\...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>2971</td>\n",
       "      <td>\" \\r\\r\\r\\n ::Given that I filed the most recen...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>2972</td>\n",
       "      <td>\" \\r\\r\\r\\n\\r\\r\\r\\n :On a different topic, the ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>2973</td>\n",
       "      <td>Like I was saying, I have absolutely no idea w...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>2974</td>\n",
       "      <td>As it stands, the sentence seems like a non se...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>2975</td>\n",
       "      <td>== a request == \\r\\r\\r\\n\\r\\r\\r\\n hello athenea...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>2976</td>\n",
       "      <td>:No problem. Wikipedia welcomes factual contri...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>2977</td>\n",
       "      <td>Thank you for making that fix. Now that the ga...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>2978</td>\n",
       "      <td>::Oh, my pleasure, thank you! — ''''''</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>2979</td>\n",
       "      <td>my senior drill instructor lost his home in Ka...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>2980</td>\n",
       "      <td>\" \\r\\r\\r\\n\\r\\r\\r\\n == Photo ID == \\r\\r\\r\\n , i...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>2981</td>\n",
       "      <td>::: And it's 2013 and I'll probably give this ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>2982</td>\n",
       "      <td>::Strangely enough, we are here to discuss imp...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>2983</td>\n",
       "      <td>, as they turn a studiously blind eye to crap ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>2984</td>\n",
       "      <td>\"==Updates== \\r\\r\\r\\n  posted this on 8/16/06:...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>2985</td>\n",
       "      <td>Re: Deletion of GlobalVAC  \\r\\r\\r\\n\\r\\r\\r\\n Gl...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>2986</td>\n",
       "      <td>::Rather than being a tool and destroying othe...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>2987</td>\n",
       "      <td>, 16 July 2006 (UTC) \\r\\r\\r\\n\\r\\r\\r\\n ::::Not ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>2988</td>\n",
       "      <td>\" \\r\\r\\r\\n ::::No, I didn't know that, I'll ha...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>2989</td>\n",
       "      <td>hey kersey! wut r u up 2??? whatever. gosh.. \\...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>2990</td>\n",
       "      <td>\" \\r\\r\\r\\n ::Thanks Pfly, I added your suggest...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>2991</td>\n",
       "      <td>\" \\r\\r\\r\\n\\r\\r\\r\\n :::Ok, pošto vidim da dobro...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>2992</td>\n",
       "      <td>FUCKING COCK AND BALLS!!!!! IM DRUNK AS A LORD</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>2993</td>\n",
       "      <td>EDES v. ELAS=== \\r\\r\\r\\n ″Eventually, the Brit...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>2994</td>\n",
       "      <td>\" \\r\\r\\r\\n :Haha, I'm so stupid... Sorry about...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>2995</td>\n",
       "      <td>:::Okay so provide the source that XYZABC was ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>2996</td>\n",
       "      <td>\"==Robert J stevens PSU== \\r\\r\\r\\n I have adde...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>2997</td>\n",
       "      <td>\" \\r\\r\\r\\n\\r\\r\\r\\n ==Wording== \\r\\r\\r\\n\\r\\r\\r\\...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>2998</td>\n",
       "      <td>== DMIA as the Premier Gateway == \\r\\r\\r\\n\\r\\r...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>2999</td>\n",
       "      <td>== ==Don’t Drain Wikipedia== == \\r\\r\\r\\n\\r\\r\\r...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>3000</td>\n",
       "      <td>== to starrbux luv twister == \\r\\r\\r\\n\\r\\r\\r\\n...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2001 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                            Comment  Labels\n",
       "0           1000  \" \\r\\r\\r\\n == Speedy deletion of \"\"Mason Henso...     0.0\n",
       "1           1001  onwhat ehbcdpyedwcdo vaeotgbcdjvfh8dwikxosmn b...     0.0\n",
       "2           1002  *I don't know what's going on here - the note ...     0.0\n",
       "3           1003  \" \\r\\r\\r\\n\\r\\r\\r\\n  \\r\\r\\r\\n == Final draft… m...     0.0\n",
       "4           1004  \" \\r\\r\\r\\n\\r\\r\\r\\n : This is not an edit propo...     1.0\n",
       "5           1005  Is he contesting 2014 Lok Sabha elecction. If ...     0.0\n",
       "6           1006  == 'citation needed' is not needed == \\r\\r\\r\\n...     0.0\n",
       "7           1007  \" \\r\\r\\r\\n\\r\\r\\r\\n  \\r\\r\\r\\n\\r\\r\\r\\n WOW, so t...     1.0\n",
       "8           1008  \" \\r\\r\\r\\n\\r\\r\\r\\n :Looking at the issue again...     0.0\n",
       "9           1009  Hi David_FLXD. I am coming back to you with re...     0.0\n",
       "10          1010  There was no warthog like that in halo 2, your...     1.0\n",
       "11          1011  I am glad we have 2010–2011 Tunisian protests....     0.0\n",
       "12          1012                        is clearly intended to make     0.0\n",
       "13          1013  Yes it's more than 1500 killed civils, and man...     0.0\n",
       "14          1014  \" \\r\\r\\r\\n :In the example I showed you, the s...     0.0\n",
       "15          1015  == Thas the reason for them to request despera...     0.0\n",
       "16          1016  ::There is now a mention in the Eastern Daily ...     0.0\n",
       "17          1017  == Sideways photo == \\r\\r\\r\\n\\r\\r\\r\\n Umm...bl...     0.0\n",
       "18          1018  You will need WP:RS sources that directly rela...     0.0\n",
       "19          1019      \" \\r\\r\\r\\n\\r\\r\\r\\n ::Thanks, Steinberger.   \"     0.0\n",
       "20          1020  2009 (UTC) \\r\\r\\r\\n ::::::I think your idea is...     0.0\n",
       "21          1021  \", 26 July 2010 (UTC) \\r\\r\\r\\n ::Can they be p...     0.0\n",
       "22          1022  OH DEAR - HOW TERRIBLE ! GIVEN THAT IT TAKES 1...     1.0\n",
       "23          1023  \" \\r\\r\\r\\n :The page was basically copied from...     0.0\n",
       "24          1024  You want to build a framework like that, find ...     0.0\n",
       "25          1025  Ethnologue SIL is not a reference concerning C...     0.0\n",
       "26          1026  I have removed what appears to me to be a spur...     0.0\n",
       "27          1027  == Hotels in Abuja == \\r\\r\\r\\n\\r\\r\\r\\n Wheneve...     0.0\n",
       "28          1028  my opinion about the Nakhcivan  Nakhcivan is a...     0.0\n",
       "29          1029  \" \\r\\r\\r\\n\\r\\r\\r\\n == Employees == \\r\\r\\r\\n\\r\\...     0.0\n",
       "...          ...                                                ...     ...\n",
       "1971        2971  \" \\r\\r\\r\\n ::Given that I filed the most recen...     NaN\n",
       "1972        2972  \" \\r\\r\\r\\n\\r\\r\\r\\n :On a different topic, the ...     NaN\n",
       "1973        2973  Like I was saying, I have absolutely no idea w...     NaN\n",
       "1974        2974  As it stands, the sentence seems like a non se...     NaN\n",
       "1975        2975  == a request == \\r\\r\\r\\n\\r\\r\\r\\n hello athenea...     NaN\n",
       "1976        2976  :No problem. Wikipedia welcomes factual contri...     NaN\n",
       "1977        2977  Thank you for making that fix. Now that the ga...     NaN\n",
       "1978        2978             ::Oh, my pleasure, thank you! — ''''''     NaN\n",
       "1979        2979  my senior drill instructor lost his home in Ka...     NaN\n",
       "1980        2980  \" \\r\\r\\r\\n\\r\\r\\r\\n == Photo ID == \\r\\r\\r\\n , i...     NaN\n",
       "1981        2981  ::: And it's 2013 and I'll probably give this ...     NaN\n",
       "1982        2982  ::Strangely enough, we are here to discuss imp...     NaN\n",
       "1983        2983  , as they turn a studiously blind eye to crap ...     NaN\n",
       "1984        2984  \"==Updates== \\r\\r\\r\\n  posted this on 8/16/06:...     NaN\n",
       "1985        2985  Re: Deletion of GlobalVAC  \\r\\r\\r\\n\\r\\r\\r\\n Gl...     NaN\n",
       "1986        2986  ::Rather than being a tool and destroying othe...     NaN\n",
       "1987        2987  , 16 July 2006 (UTC) \\r\\r\\r\\n\\r\\r\\r\\n ::::Not ...     NaN\n",
       "1988        2988  \" \\r\\r\\r\\n ::::No, I didn't know that, I'll ha...     NaN\n",
       "1989        2989  hey kersey! wut r u up 2??? whatever. gosh.. \\...     NaN\n",
       "1990        2990  \" \\r\\r\\r\\n ::Thanks Pfly, I added your suggest...     NaN\n",
       "1991        2991  \" \\r\\r\\r\\n\\r\\r\\r\\n :::Ok, pošto vidim da dobro...     NaN\n",
       "1992        2992     FUCKING COCK AND BALLS!!!!! IM DRUNK AS A LORD     NaN\n",
       "1993        2993  EDES v. ELAS=== \\r\\r\\r\\n ″Eventually, the Brit...     NaN\n",
       "1994        2994  \" \\r\\r\\r\\n :Haha, I'm so stupid... Sorry about...     NaN\n",
       "1995        2995  :::Okay so provide the source that XYZABC was ...     NaN\n",
       "1996        2996  \"==Robert J stevens PSU== \\r\\r\\r\\n I have adde...     NaN\n",
       "1997        2997  \" \\r\\r\\r\\n\\r\\r\\r\\n ==Wording== \\r\\r\\r\\n\\r\\r\\r\\...     NaN\n",
       "1998        2998  == DMIA as the Premier Gateway == \\r\\r\\r\\n\\r\\r...     NaN\n",
       "1999        2999  == ==Don’t Drain Wikipedia== == \\r\\r\\r\\n\\r\\r\\r...     NaN\n",
       "2000        3000  == to starrbux luv twister == \\r\\r\\r\\n\\r\\r\\r\\n...     NaN\n",
       "\n",
       "[2001 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_df = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test_df = pd.read_csv(TEST_DATA_FILE)\n",
    "human_df = pd.read_csv('../Terminator Mode/test_data_labelled')\n",
    "human_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Total 2195895 word vectors.\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## index word vectors\n",
    "########################################\n",
    "print('Indexing word vectors')\n",
    "\n",
    "#Glove Vectors\n",
    "embeddings_index = {}\n",
    "f = open(EMBEDDING_FILE, encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = ' '.join(values[:-300])\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## process texts in datasets\n",
    "########################################\n",
    "print('Processing text dataset')\n",
    "\n",
    "#Regex to remove all Non-Alpha Numeric and space\n",
    "special_character_removal=re.compile(r'[^a-z\\d ]',re.IGNORECASE)\n",
    "\n",
    "#regex to replace all numerics\n",
    "replace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n",
    "\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    #Remove Special Characters\n",
    "    text=special_character_removal.sub('',text)\n",
    "    \n",
    "    #Replace Numbers\n",
    "    text=replace_numbers.sub('n',text)\n",
    "\n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.0\n",
       "1      0.0\n",
       "2      0.0\n",
       "3      0.0\n",
       "4      1.0\n",
       "5      0.0\n",
       "6      0.0\n",
       "7      1.0\n",
       "8      0.0\n",
       "9      0.0\n",
       "10     1.0\n",
       "11     0.0\n",
       "12     0.0\n",
       "13     0.0\n",
       "14     0.0\n",
       "15     0.0\n",
       "16     0.0\n",
       "17     0.0\n",
       "18     0.0\n",
       "19     0.0\n",
       "20     0.0\n",
       "21     0.0\n",
       "22     1.0\n",
       "23     0.0\n",
       "24     0.0\n",
       "25     0.0\n",
       "26     0.0\n",
       "27     0.0\n",
       "28     0.0\n",
       "29     0.0\n",
       "      ... \n",
       "970    0.0\n",
       "971    0.0\n",
       "972    0.0\n",
       "973    0.0\n",
       "974    1.0\n",
       "975    0.0\n",
       "976    0.0\n",
       "977    0.0\n",
       "978    0.0\n",
       "979    0.0\n",
       "980    0.0\n",
       "981    0.0\n",
       "982    0.0\n",
       "983    1.0\n",
       "984    0.0\n",
       "985    1.0\n",
       "986    0.0\n",
       "987    0.0\n",
       "988    0.0\n",
       "989    0.0\n",
       "990    0.0\n",
       "991    0.0\n",
       "992    0.0\n",
       "993    0.0\n",
       "994    0.0\n",
       "995    0.0\n",
       "996    0.0\n",
       "997    0.0\n",
       "998    1.0\n",
       "999    0.0\n",
       "Name: Labels, Length: 1000, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train = train_df[\"Comment\"].fillna(\"NA\").values\n",
    "y = train_df[\"Labels\"].values\n",
    "list_sentences_test = test_df[\"Comment\"].fillna(\"NA\").values\n",
    "y_real = test_df[\"Labels\"]\n",
    "list_sentences_human = human_df[\"Comment\"][0:1000]\n",
    "y_human = human_df[\"Labels\"][0:1000]\n",
    "y_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 271583 unique tokens\n",
      "Shape of data tensor: (159571, 200)\n",
      "Shape of label tensor: (159571,)\n",
      "Shape of test_data tensor: (63978, 200)\n",
      "Shape of human data tensor: (1000, 200)\n"
     ]
    }
   ],
   "source": [
    "comments = []\n",
    "for text in list_sentences_train:\n",
    "    comments.append(text_to_wordlist(text))\n",
    "    \n",
    "test_comments=[]\n",
    "for text in list_sentences_test:\n",
    "    test_comments.append(text_to_wordlist(text))\n",
    "\n",
    "human_comments = []\n",
    "for text in list_sentences_human:\n",
    "    human_comments.append(text_to_wordlist(text))\n",
    "    \n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(comments + test_comments)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_comments)\n",
    "human_sequences = tokenizer.texts_to_sequences(human_comments)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor:', test_data.shape)\n",
    "human_data = pad_sequences(human_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of human data tensor:', human_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 5051\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## prepare embeddings\n",
    "########################################\n",
    "print('Preparing embedding matrix')\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(135635, 200) (135635,)\n",
      "(23936, 200) (23936,)\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## sample train/validation data\n",
    "########################################\n",
    "# np.random.seed(1234)\n",
    "perm = np.random.permutation(len(data))\n",
    "idx_train = perm[:int(len(data)*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(len(data)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_train=data[idx_train]\n",
    "labels_train=y[idx_train]\n",
    "print(data_train.shape,labels_train.shape)\n",
    "\n",
    "data_val=data[idx_val]\n",
    "labels_val=y[idx_val]\n",
    "\n",
    "print(data_val.shape,labels_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## define the model structure\n",
    "########################################\n",
    "embedding_layer = Embedding(nb_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)\n",
    "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm,return_sequences=True)\n",
    "\n",
    "comment_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences= embedding_layer(comment_input)\n",
    "x = lstm_layer(embedded_sequences)\n",
    "x = Dropout(rate_drop_dense)(x)\n",
    "merged = Attention(MAX_SEQUENCE_LENGTH)(x)\n",
    "merged = Dense(num_dense, activation=act)(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 200, 300)          12000000  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200, 200)          400800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200, 200)          0         \n",
      "_________________________________________________________________\n",
      "attention_1 (Attention)      (None, 200)               400       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               51456     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 12,453,937\n",
      "Trainable params: 453,425\n",
      "Non-trainable params: 12,000,512\n",
      "_________________________________________________________________\n",
      "None\n",
      "simple_lstm_glove_vectors_0.40_0.40\n",
      "Train on 135635 samples, validate on 23936 samples\n",
      "Epoch 1/10\n",
      "135635/135635 [==============================] - 377s 3ms/step - loss: 0.1585 - acc: 0.9445 - val_loss: 0.1160 - val_acc: 0.9573\n",
      "Epoch 2/10\n",
      "135635/135635 [==============================] - 477s 4ms/step - loss: 0.1189 - acc: 0.9570 - val_loss: 0.1034 - val_acc: 0.9629\n",
      "Epoch 3/10\n",
      "135635/135635 [==============================] - 495s 4ms/step - loss: 0.1129 - acc: 0.9586 - val_loss: 0.1005 - val_acc: 0.9637\n",
      "Epoch 4/10\n",
      "135635/135635 [==============================] - 638s 5ms/step - loss: 0.1088 - acc: 0.9607 - val_loss: 0.1007 - val_acc: 0.9630\n",
      "Epoch 5/10\n",
      " 62080/135635 [============>.................] - ETA: 3:16 - loss: 0.1055 - acc: 0.9615"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## train the model\n",
    "########################################\n",
    "model = Model(inputs=[comment_input], \\\n",
    "        outputs=preds)\n",
    "adam = Adam(lr=0.00001, beta_1=0.9, beta_2=0.99, epsilon=1e-8)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "STAMP = 'simple_lstm_glove_vectors_%.2f_%.2f'%(rate_drop_lstm,rate_drop_dense)\n",
    "print(STAMP)\n",
    "bst_model_path = STAMP + '.h5'\n",
    "\n",
    "#early_stopping =EarlyStopping(monitor='val_loss', patience=10)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min')\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "hist = model.fit(data_train, labels_train, \\\n",
    "        validation_data=(data_val, labels_val), \\\n",
    "        epochs=num_epochs, batch_size=batch_size, shuffle=True, \\\n",
    "         callbacks=[model_checkpoint, reduce_lr])\n",
    "model.load_weights(bst_model_path)\n",
    "bst_val_score = min(hist.history['val_loss'])\n",
    "y_test = model.predict([test_data], batch_size=1024, verbose=1)\n",
    "y_human_pred = model.predict([human_data], batch_size=1000, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "y_test = np.where(y_test > 0.5, 1, 0)\n",
    "confmat = confusion_matrix(y_real, y_test)\n",
    "print(confmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = roc_auc_score(y_real, y_test, 'macro')\n",
    "print(\"ROC AUC score mean: %f\" % (roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot training and validation loss\n",
    "fig = plt.figure(figsize= (15,10), dpi=80)\n",
    "fig.subplots_adjust(hspace = 0.4, wspace = 0.4)\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.arange(num_epochs), hist.history['val_loss'], label=\"Validation Loss\")\n",
    "plt.plot( np.arange(num_epochs), hist.history['loss'], label=\"Training loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Binary Cross Entropy Loss\")\n",
    "plt.ylim((0, 0.3))\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(np.arange(num_epochs), hist.history['val_acc'], label =\"Validation Accuracy\")\n",
    "plt.plot(np.arange(num_epochs), hist.history['acc'], label = \"Training Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim((0.9, 1))\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "cm = pd.DataFrame(confmat);\n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "print(\"ROC AUC score mean: %f\" % (roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "y_human_pred = np.where(y_human_pred > 0.5, 1, 0)\n",
    "confmat = confusion_matrix(y_human, y_human_pred)\n",
    "print(confmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = roc_auc_score(y_human, y_human_pred, 'macro')\n",
    "confmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "cm = pd.DataFrame(confmat);\n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "print(\"ROC AUC score mean: %f\" % (roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
